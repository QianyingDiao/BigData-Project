{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Bosch Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-21-112.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Bosch Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8d042dc908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "num_new_pd = pd.read_csv('s3://jiaruxu233/new_train_numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.read_csv('s3://jiaruxu233/new_train_numerical.csv',usecols=['Response']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_new_pd = pd.concat([num_new_pd,response],axis=1).to_csv('s3://jiaruxu233/new_train_numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_new = spark.read\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .format('csv')\\\n",
    "  .load('s3://jiaruxu233/new_train_numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_new = spark.read\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .format('csv')\\\n",
    "  .load('s3://jiaruxu233/train_categorical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_new = spark.read\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .format('csv')\\\n",
    "  .load('s3://jiaruxu233/train_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "new_train = num_new.join(cat_new, [\"_c0\"]).join(date_new, [\"_c0\"]).drop('_c0').drop('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler,Binarizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml import Pipeline, Model\n",
    "from pyspark.sql.functions import to_timestamp, year, month, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somehow we have duplicate response columns. Therefore, we drop the unnecessary one.\n",
    "new_train = new_train.drop('Response104')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename response column for convenience\n",
    "new_train = new_train.withColumnRenamed(\"Response103\", \"Response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in accordance with our previous engineering section\n",
    "new_train = new_train.na.fill(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_and_date_feature:\n",
    "    new_train = new_train.withColumn(col,new_train[col].cast('float'))\n",
    "# new_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 947155\n",
      "Number of testing records : 213200\n",
      "Number of prediction records : 23392\n"
     ]
    }
   ],
   "source": [
    "splitted_data = new_train.randomSplit([0.8, 0.18, 0.02], 666)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))\n",
    "print(\"Number of prediction records : \" + str(predict_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.columns[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the needed numerical and date columns \n",
    "num_and_date_feature = new_train.columns[1:101]+new_train.columns[111:]\n",
    "len(num_and_date_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_and_date_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringIndexer\n",
    "stringIndexer_L1_S24_F1114 = StringIndexer(inputCol=\"L1_S24_F1114\", outputCol=\"L1_S24_F1114_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L1_S24_F1137 = StringIndexer(inputCol=\"L1_S24_F1137\", outputCol=\"L1_S24_F1137_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L1_S24_F1140 = StringIndexer(inputCol=\"L1_S24_F1140\", outputCol=\"L1_S24_F1140_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L1_S24_F1510 = StringIndexer(inputCol=\"L1_S24_F1510\", outputCol=\"L1_S24_F1510_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L1_S24_F1525 = StringIndexer(inputCol=\"L1_S24_F1525\", outputCol=\"L1_S24_F1525_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L1_S24_F1530 = StringIndexer(inputCol=\"L1_S24_F1530\", outputCol=\"L1_S24_F1530_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L1_S25_F1852 = StringIndexer(inputCol=\"L1_S25_F1852\", outputCol=\"L1_S25_F1852_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L1_S25_F2779 = StringIndexer(inputCol=\"L1_S25_F2779\", outputCol=\"L1_S25_F2779_IX\",handleInvalid=\"keep\")\n",
    "stringIndexer_L3_S32_F3854 = StringIndexer(inputCol=\"L3_S32_F3854\", outputCol=\"L3_S32_F3854_IX\",handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(\n",
    "    inputCols = [col for col in num_and_date_feature] + [\"L1_S24_F1114_IX\",\"L1_S24_F1137_IX\",\"L1_S24_F1140_IX\",\n",
    "              \"L1_S24_F1510_IX\",\"L1_S24_F1525_IX\",\"L1_S24_F1530_IX\",\"L1_S25_F1852_IX\",\n",
    "               \"L1_S25_F2779_IX\",\"L3_S32_F3854_IX\"],\n",
    "    outputCol=\"features\",handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_b87992bc41da"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorAssembler_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(labelCol=\"Response\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logit = Pipeline(stages=[stringIndexer_L1_S24_F1114, stringIndexer_L1_S24_F1137, \n",
    "                               stringIndexer_L1_S24_F1140, stringIndexer_L1_S24_F1510,\n",
    "                               stringIndexer_L1_S24_F1525, stringIndexer_L1_S24_F1530,\n",
    "                               stringIndexer_L1_S25_F1852, stringIndexer_L1_S25_F2779,\n",
    "                               stringIndexer_L3_S32_F3854,\n",
    "                               vectorAssembler_features, \n",
    "                               logit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit = pipeline_logit.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logit = model_logit.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|       0.0|213198|\n",
      "|       1.0|     2|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_logit.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pred = predictions_logit.select('prediction','Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Response| count|\n",
      "+--------+------+\n",
      "|     1.0|  1248|\n",
      "|     0.0|211952|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The real structure for response in our test dataset\n",
    "test_data.groupBy('Response').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pred.filter(\"Response == 1 \").filter(\"prediction == 1\").count() # True Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1247"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pred.filter(\"Response == 1 \").filter(\"prediction == 0\").count() # False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pred.filter(\"Response == 0 \").filter(\"prediction == 1\").count() # False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211951"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pred.filter(\"Response == 0 \").filter(\"prediction == 0\").count() # True Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01983993227463039"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tp = 1\n",
    "fp = 1\n",
    "fn = 1247\n",
    "tn = 211952\n",
    "mcc = (tp*tn - fp*fn) /( math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions.select('prediction','Response').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictionAndLabels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "# evaluatorLogit = BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluatorLogit.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "# rf = RandomForestClassifier(labelCol=\"Response\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_rf = Pipeline(stages=[stringIndexer_L1_S24_F1114, stringIndexer_L1_S24_F1137, \n",
    "#                                stringIndexer_L1_S24_F1140, stringIndexer_L1_S24_F1510,\n",
    "#                                stringIndexer_L1_S24_F1525, stringIndexer_L1_S24_F1530,\n",
    "#                                stringIndexer_L1_S25_F1852, stringIndexer_L1_S25_F2779,\n",
    "#                                stringIndexer_L3_S32_F3854,\n",
    "#                                vectorAssembler_features, \n",
    "#                                rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_rf = pipeline_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model_rf.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictionAndLabels_rf = predictions.select('prediction','Response').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluatorRF = BinaryClassificationMetrics(predictionAndLabels_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluatorRF.areaUnderROC -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.groupBy('prediction').count().show()  - prediction has no 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|       0.0|213148|\n",
      "|       1.0|    52|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"Response\", featuresCol=\"features\")\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline_gbt = Pipeline(stages=[stringIndexer_L1_S24_F1114, stringIndexer_L1_S24_F1137, \n",
    "                               stringIndexer_L1_S24_F1140, stringIndexer_L1_S24_F1510,\n",
    "                               stringIndexer_L1_S24_F1525, stringIndexer_L1_S24_F1530,\n",
    "                               stringIndexer_L1_S25_F1852, stringIndexer_L1_S25_F2779,\n",
    "                               stringIndexer_L3_S32_F3854,\n",
    "                               vectorAssembler_features, \n",
    "                               gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "gbt_model = pipeline_gbt.fit(train_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_gbt = gbt_model.transform(test_data)\n",
    "predictions_gbt.groupBy('prediction').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the count table above, we notice that we are doing a far more better job than the previous two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_pred = predictions_gbt.select('prediction','Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_pred.filter(\"Response == 1 \").filter(\"prediction == 1\").count() # True Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_pred.filter(\"Response == 1 \").filter(\"prediction == 0\").count() # False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_pred.filter(\"Response == 0 \").filter(\"prediction == 1\").count() # False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211925"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_pred.filter(\"Response == 0 \").filter(\"prediction == 0\").count() # True Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09723857799358827"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tp = 25\n",
    "fp = 27\n",
    "fn = 1223\n",
    "tn = 211925\n",
    "mcc = (tp*tn - fp*fn) /( math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num =spark.read.csv('s3://jiaruxu233/train_numeric.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = spark.read.csv('s3://jiaruxu233/train_categorical.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = spark.read.csv('s3://jiaruxu233/train_date.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_feature = ['L1_S24_F1078',\n",
    " 'L1_S24_F833',\n",
    " 'L0_S2_F63',\n",
    " 'L0_S2_F67',\n",
    " 'L1_S24_F866',\n",
    " 'L1_S24_F838',\n",
    " 'L1_S24_F849',\n",
    " 'L1_S24_F824',\n",
    " 'L1_S24_F873',\n",
    " 'L1_S24_F817',\n",
    " 'L1_S24_F825',\n",
    " 'L1_S24_F871',\n",
    " 'L1_S24_F1095',\n",
    " 'L1_S24_F845',\n",
    " 'L1_S24_F856',\n",
    " 'L0_S2_F35',\n",
    " 'L1_S24_F828',\n",
    " 'L0_S2_F43',\n",
    " 'L0_S2_F47',\n",
    " 'L0_S2_F39',\n",
    " 'L1_S24_F868',\n",
    " 'L0_S2_F51',\n",
    " 'L1_S24_F1061',\n",
    " 'L1_S24_F861',\n",
    " 'L1_S24_F1105',\n",
    " 'L1_S24_F1069',\n",
    " 'L1_S24_F830',\n",
    " 'L1_S24_F863',\n",
    " 'L1_S24_F898',\n",
    " 'L1_S24_F1082',\n",
    " 'L1_S24_F820',\n",
    " 'L1_S24_F1097',\n",
    " 'L1_S24_F835',\n",
    " 'L1_S24_F896',\n",
    " 'L1_S24_F821',\n",
    " 'L1_S24_F1608',\n",
    " 'L0_S2_F55',\n",
    " 'L1_S24_F852',\n",
    " 'L1_S24_F858',\n",
    " 'L0_S10_F270']\n",
    "\n",
    "d_feature = ['L3_S37_D3949','L3_S30_D3541','L3_S34_D3877','L3_S33_D3870']\n",
    "n_feature = ['L3_S33_F3865',\n",
    " 'L3_S33_F3857',\n",
    " 'L3_S29_F3342',\n",
    " 'L3_S29_F3412',\n",
    " 'L3_S29_F3351',\n",
    " 'L3_S29_F3470',\n",
    " 'L3_S29_F3339',\n",
    " 'L3_S29_F3479',\n",
    " 'L3_S29_F3458',\n",
    " 'L0_S0_F0',\n",
    " 'L3_S29_F3455',\n",
    " 'L3_S29_F3449',\n",
    " 'L3_S29_F3327',\n",
    " 'L3_S29_F3407',\n",
    " 'L3_S30_F3754',\n",
    " 'L3_S30_F3759']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Subset train_date set(pick up only the unique date columns)\n",
    "date_columns = ['Id','L3_S37_D3949', 'L3_S30_D3541', 'L3_S29_D3428', 'L3_S34_D3877', 'L3_S33_D3870', 'L0_S0_D3', 'L0_S1_D30', 'L0_S8_D145', 'L3_S36_D3919', 'L3_S35_D3910', 'L0_S5_D115', 'L0_S3_D70', 'L0_S6_D120', 'L0_S7_D143', 'L0_S2_D54', 'L0_S4_D106', 'L0_S12_D333', 'L0_S20_D465', 'L0_S13_D355', 'L0_S10_D266', 'L0_S11_D284', 'L0_S9_D192', 'L2_S26_D3084', 'L1_S24_D1828', 'L0_S19_D457', 'L0_S17_D432', 'L0_S14_D380', 'L0_S15_D401', 'L0_S16_D428', 'L2_S27_D3156', 'L0_S18_D447', 'L0_S21_D469', 'L0_S23_D629', 'L0_S22_D608', 'L3_S41_D4021', 'L3_S40_D3981', 'L3_S45_D4129', 'L3_S48_D4203', 'L3_S47_D4155', 'L3_S39_D3974', 'L3_S51_D4255', 'L1_S25_D1887', 'L3_S31_D3848', 'L3_S43_D4097', 'L3_S49_D4208', 'L3_S50_D4254', 'L3_S44_D4122', 'L3_S38_D3953', 'L3_S32_D3852', 'L2_S28_D3234', 'L3_S46_D4135', 'L3_S42_D4057']\n",
    "new_date = train_date.select(date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = train_num.select(n_feature+['Id','Response']).join(new_date.select(['Id']+d_feature), 'Id').join(train_cat.select(['Id']+c_feature), 'Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, L3_S33_F3865: double, L3_S33_F3857: double, L3_S29_F3342: double, L3_S29_F3412: double, L3_S29_F3351: double, L3_S29_F3470: double, L3_S29_F3339: double, L3_S29_F3479: double, L3_S29_F3458: double, L0_S0_F0: double, L3_S29_F3455: double, L3_S29_F3449: double, L3_S29_F3327: double, L3_S29_F3407: double, L3_S30_F3754: double, L3_S30_F3759: double, Response: int, L3_S37_D3949: double, L3_S30_D3541: double, L3_S34_D3877: double, L3_S33_D3870: double, L1_S24_F1078: string, L1_S24_F833: string, L0_S2_F63: string, L0_S2_F67: string, L1_S24_F866: string, L1_S24_F838: string, L1_S24_F849: string, L1_S24_F824: string, L1_S24_F873: string, L1_S24_F817: string, L1_S24_F825: string, L1_S24_F871: string, L1_S24_F1095: string, L1_S24_F845: string, L1_S24_F856: string, L0_S2_F35: string, L1_S24_F828: string, L0_S2_F43: string, L0_S2_F47: string, L0_S2_F39: string, L1_S24_F868: string, L0_S2_F51: string, L1_S24_F1061: string, L1_S24_F861: string, L1_S24_F1105: string, L1_S24_F1069: string, L1_S24_F830: string, L1_S24_F863: string, L1_S24_F898: string, L1_S24_F1082: string, L1_S24_F820: string, L1_S24_F1097: string, L1_S24_F835: string, L1_S24_F896: string, L1_S24_F821: string, L1_S24_F1608: string, L0_S2_F55: string, L1_S24_F852: string, L1_S24_F858: string, L0_S10_F270: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = new_df.randomSplit([0.8, 0.2], 1234)\n",
    "train_data = split[0]\n",
    "test_data = split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_IX\",\\\n",
    "                          handleInvalid=\"keep\") for column in c_feature]\n",
    "\n",
    "vectorAssembler_features = VectorAssembler(inputCols=[x+\"_IX\" for x in c_feature]+d_feature+n_feature, outputCol=\"features\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Response\", featuresCol=\"features\")\n",
    "pipeline_rf = Pipeline(stages= indexers + [vectorAssembler_features, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pipeline_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives= 0; True Negatives= 235286; False Positives= 0; False Negatives= 1331\n"
     ]
    }
   ],
   "source": [
    "# True Positives\n",
    "tp= predictions[(predictions.Response == 1) & (predictions.prediction == 1)].count()\n",
    "# True Negatives\n",
    "tn= predictions[(predictions.Response == 0) & (predictions.prediction == 0)].count()\n",
    "# False Positives\n",
    "fp= predictions[(predictions.Response == 0) & (predictions.prediction == 1)].count()\n",
    "# False Negatives\n",
    "fn= predictions[(predictions.Response == 1) & (predictions.prediction == 0)].count()\n",
    "   \n",
    "print (\"True Positives= %s; True Negatives= %s; False Positives= %s; False Negatives= %s\" %(tp, tn, fp, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-26f5e618a640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# mcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ZeroDivisionError: float division by zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import math\n",
    "mcc = (tp*tn - fp*fn) /( math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "# mcc\n",
    "# ZeroDivisionError: float division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Gradient Boosted Trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|       0.0|236611|\n",
      "|       1.0|     6|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"Response\", featuresCol=\"features\")\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "pipeline_gbt = Pipeline(stages= indexers + [vectorAssembler_features, gbt])\n",
    "gbt_model = pipeline_gbt.fit(train_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_gbt = gbt_model.transform(test_data)\n",
    "predictions_gbt.groupBy('prediction').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives= 2; True Negatives= 235282; False Positives= 4; False Negatives= 1329\n"
     ]
    }
   ],
   "source": [
    "tp= predictions_gbt[(predictions_gbt.Response == 1) & (predictions_gbt.prediction == 1)].count()\n",
    "# True Negatives\n",
    "tn= predictions_gbt[(predictions_gbt.Response == 0) & (predictions_gbt.prediction == 0)].count()\n",
    "# False Positives\n",
    "fp= predictions_gbt[(predictions_gbt.Response == 0) & (predictions_gbt.prediction == 1)].count()\n",
    "# False Negatives\n",
    "fn= predictions_gbt[(predictions_gbt.Response == 1) & (predictions_gbt.prediction == 0)].count()\n",
    "\n",
    "print (\"True Positives= %s; True Negatives= %s; False Positives= %s; False Negatives= %s\" %(tp, tn, fp, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02206502140871241"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "mcc = (tp*tn - fp*fn) /( math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, in modeling part, we have gone through different classfication methods, such as **Logistic Regression, Random Forest and Gradient-boosted tree classifier**. According to each result, we find out that **Gradient-boosted tree** has the best predictability. As for evaluation method, we choose **Matthews correlation coefficient** to prevent the effect of data imbalance. We adopt classfication metric to look into in what proportion we are doing things right and making mistakes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
